# This file was auto-generated by Fern from our API Definition.

import typing

from ...core.client_wrapper import AsyncClientWrapper, SyncClientWrapper
from ...core.request_options import RequestOptions
from ...types.audio_input_request import AudioInputRequest
from ...types.embedding_success_response import EmbeddingSuccessResponse
from ...types.image_input_request import ImageInputRequest
from ...types.text_image_input_request import TextImageInputRequest
from ...types.text_input_request import TextInputRequest
from ...types.video_input_request import VideoInputRequest
from .raw_client import AsyncRawV2Client, RawV2Client
from .tasks.client import AsyncTasksClient, TasksClient
from .types.create_embeddings_request_input_type import CreateEmbeddingsRequestInputType
from .types.create_embeddings_request_model_name import CreateEmbeddingsRequestModelName

# this is used as the default value for optional parameters
OMIT = typing.cast(typing.Any, ...)


class V2Client:
    def __init__(self, *, client_wrapper: SyncClientWrapper):
        self._raw_client = RawV2Client(client_wrapper=client_wrapper)
        self.tasks = TasksClient(client_wrapper=client_wrapper)

    @property
    def with_raw_response(self) -> RawV2Client:
        """
        Retrieves a raw implementation of this client that returns raw responses.

        Returns
        -------
        RawV2Client
        """
        return self._raw_client

    def create(
        self,
        *,
        input_type: CreateEmbeddingsRequestInputType,
        model_name: CreateEmbeddingsRequestModelName,
        text: typing.Optional[TextInputRequest] = OMIT,
        image: typing.Optional[ImageInputRequest] = OMIT,
        text_image: typing.Optional[TextImageInputRequest] = OMIT,
        audio: typing.Optional[AudioInputRequest] = OMIT,
        video: typing.Optional[VideoInputRequest] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> EmbeddingSuccessResponse:
        """
        This endpoint synchronously creates embeddings for multimodal content and returns the results immediately in the response.

        <Note title="Note">
          This method only supports Marengo version 3.0 or newer.
        </Note>

        **When to use this endpoint**:
        - Create embeddings for text, images, audio, or video content
        - Get immediate results without waiting for background processing
        - Process audio or video content up to 10 minutes in duration

        **Do not use this endpoint for**:
        - Audio or video content longer than 10 minutes. Use the [`POST`](/v1.3/api-reference/create-embeddings-v2/create-async-embedding-task) method of the `/embed-v2/tasks` endpoint instead.

        <Accordion title="Input requirements">
          **Text**:
          - Maximum length: 500 tokens

          **Images**:
          - Formats: JPEG, PNG
          - Minimum size: 128x128 pixels
          - Maximum file size: 5 MB

          **Audio and video**:
          - Maximum duration: 10 minutes
          - Maximum file size for base64 encoded strings: 36 MB
          - Audio formats: WAV (uncompressed), MP3 (lossy), FLAC (lossless)
          - Video formats: [FFmpeg supported formats](https://ffmpeg.org/ffmpeg-formats.html)
          - Video resolution: 360x360 to 5184x2160 pixels
          - Aspect ratio: Between 1:1 and 1:2.4, or between 2.4:1 and 1:1
        </Accordion>

        Parameters
        ----------
        input_type : CreateEmbeddingsRequestInputType
            The type of content for the embeddings.


            **Values**:
            - `audio`: Creates embeddings for an audio file
            - `video`: Creates embeddings for a video file
            - `image`: Creates embeddings for an image file
            - `text`: Creates embeddings for text input
            - `text_image`: Creates embeddings for text and an image.

        model_name : CreateEmbeddingsRequestModelName
            The video understanding model to use. Only "marengo3.0" is supported.

        text : typing.Optional[TextInputRequest]

        image : typing.Optional[ImageInputRequest]

        text_image : typing.Optional[TextImageInputRequest]

        audio : typing.Optional[AudioInputRequest]

        video : typing.Optional[VideoInputRequest]

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        EmbeddingSuccessResponse
            Successful request; normal operation

        Examples
        --------
        from twelvelabs import TextInputRequest, TwelveLabs

        client = TwelveLabs(
            api_key="YOUR_API_KEY",
        )
        client.embed.v_2.create(
            input_type="text",
            model_name="marengo3.0",
            text=TextInputRequest(
                input_text="man walking a dog",
            ),
        )
        """
        _response = self._raw_client.create(
            input_type=input_type,
            model_name=model_name,
            text=text,
            image=image,
            text_image=text_image,
            audio=audio,
            video=video,
            request_options=request_options,
        )
        return _response.data


class AsyncV2Client:
    def __init__(self, *, client_wrapper: AsyncClientWrapper):
        self._raw_client = AsyncRawV2Client(client_wrapper=client_wrapper)
        self.tasks = AsyncTasksClient(client_wrapper=client_wrapper)

    @property
    def with_raw_response(self) -> AsyncRawV2Client:
        """
        Retrieves a raw implementation of this client that returns raw responses.

        Returns
        -------
        AsyncRawV2Client
        """
        return self._raw_client

    async def create(
        self,
        *,
        input_type: CreateEmbeddingsRequestInputType,
        model_name: CreateEmbeddingsRequestModelName,
        text: typing.Optional[TextInputRequest] = OMIT,
        image: typing.Optional[ImageInputRequest] = OMIT,
        text_image: typing.Optional[TextImageInputRequest] = OMIT,
        audio: typing.Optional[AudioInputRequest] = OMIT,
        video: typing.Optional[VideoInputRequest] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> EmbeddingSuccessResponse:
        """
        This endpoint synchronously creates embeddings for multimodal content and returns the results immediately in the response.

        <Note title="Note">
          This method only supports Marengo version 3.0 or newer.
        </Note>

        **When to use this endpoint**:
        - Create embeddings for text, images, audio, or video content
        - Get immediate results without waiting for background processing
        - Process audio or video content up to 10 minutes in duration

        **Do not use this endpoint for**:
        - Audio or video content longer than 10 minutes. Use the [`POST`](/v1.3/api-reference/create-embeddings-v2/create-async-embedding-task) method of the `/embed-v2/tasks` endpoint instead.

        <Accordion title="Input requirements">
          **Text**:
          - Maximum length: 500 tokens

          **Images**:
          - Formats: JPEG, PNG
          - Minimum size: 128x128 pixels
          - Maximum file size: 5 MB

          **Audio and video**:
          - Maximum duration: 10 minutes
          - Maximum file size for base64 encoded strings: 36 MB
          - Audio formats: WAV (uncompressed), MP3 (lossy), FLAC (lossless)
          - Video formats: [FFmpeg supported formats](https://ffmpeg.org/ffmpeg-formats.html)
          - Video resolution: 360x360 to 5184x2160 pixels
          - Aspect ratio: Between 1:1 and 1:2.4, or between 2.4:1 and 1:1
        </Accordion>

        Parameters
        ----------
        input_type : CreateEmbeddingsRequestInputType
            The type of content for the embeddings.


            **Values**:
            - `audio`: Creates embeddings for an audio file
            - `video`: Creates embeddings for a video file
            - `image`: Creates embeddings for an image file
            - `text`: Creates embeddings for text input
            - `text_image`: Creates embeddings for text and an image.

        model_name : CreateEmbeddingsRequestModelName
            The video understanding model to use. Only "marengo3.0" is supported.

        text : typing.Optional[TextInputRequest]

        image : typing.Optional[ImageInputRequest]

        text_image : typing.Optional[TextImageInputRequest]

        audio : typing.Optional[AudioInputRequest]

        video : typing.Optional[VideoInputRequest]

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        EmbeddingSuccessResponse
            Successful request; normal operation

        Examples
        --------
        import asyncio

        from twelvelabs import AsyncTwelveLabs, TextInputRequest

        client = AsyncTwelveLabs(
            api_key="YOUR_API_KEY",
        )


        async def main() -> None:
            await client.embed.v_2.create(
                input_type="text",
                model_name="marengo3.0",
                text=TextInputRequest(
                    input_text="man walking a dog",
                ),
            )


        asyncio.run(main())
        """
        _response = await self._raw_client.create(
            input_type=input_type,
            model_name=model_name,
            text=text,
            image=image,
            text_image=text_image,
            audio=audio,
            video=video,
            request_options=request_options,
        )
        return _response.data
